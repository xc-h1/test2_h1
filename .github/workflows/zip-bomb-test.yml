name: Zip Bomb Upload Workflow

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  unzip-and-upload:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout the repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # Step 2: Install unzip utility
      - name: Install unzip
        run: sudo apt-get install -y unzip

      # Step 3: List files in the ZIP
      - name: List files in ZIP
        id: list_files
        run: |
          if unzip -l zbbig2.zip | awk '/assets\// {print $NF}' > file_list.txt; then
            echo "Files listed successfully."
            cat file_list.txt
          else
            echo "Failed to list files in zip."
            exit 1
          fi

      # Step 4: Unzip files in parallel batches and commit
      - name: Unzip in parallel batches and commit
        run: |
          # Define the batch size for parallel processing
          batch_size=5

          # Read file list into an array
          mapfile -t files < file_list.txt

          # Function to unzip and commit files
          extract_and_commit() {
            local file="$1"
            if unzip -o zbbig2.zip "$file"; then
              echo "$file extracted successfully."
              
              # Set up Git user for committing
              git config --global user.name 'github-actions[bot]'
              git config --global user.email 'github-actions[bot]@users.noreply.github.com'

              # Commit and push the extracted file
              if [ -f "$file" ]; then
                git add "$file"
                git commit -m "Add extracted file: $file"
                git push || echo "Push failed for $file"
                # Delete the file after committing
                rm "$file" || echo "Failed to delete $file"
              fi
            else
              echo "Failed to extract $file"
            fi
          }

          # Export the function so xargs can access it
          export -f extract_and_commit

          # Process files in parallel batches
          total_files="${#files[@]}"
          index=0

          while [ $index -lt $total_files ]; do
            echo "Processing batch starting from index $index"

            # Get the next batch of files
            batch_files=("${files[@]:index:batch_size}")
            
            # Run extraction and commit in parallel for the batch
            echo "${batch_files[@]}" | xargs -P $batch_size -I {} bash -c 'extract_and_commit "$@"' _ {}

            # Pause and wait for parallel jobs to finish
            wait

            # Increment index by batch size for next batch
            index=$((index + batch_size))
          done

      # Step 5: Clean up
      - name: Clean up
        run: rm -f file_list.txt
